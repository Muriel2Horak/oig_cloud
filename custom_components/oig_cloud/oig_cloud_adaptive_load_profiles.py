"""Sensor pro automatickou tvorbu adaptivn√≠ch profil≈Ø spot≈ôeby z historick√Ωch dat."""

import logging
import asyncio
import numpy as np
from typing import Any, Dict, List, Optional
from datetime import datetime, timedelta

from homeassistant.components.sensor import SensorEntity
from homeassistant.helpers.update_coordinator import CoordinatorEntity
from homeassistant.const import EntityCategory
from homeassistant.core import HomeAssistant
from homeassistant.config_entries import ConfigEntry
from homeassistant.util import dt as dt_util

_LOGGER = logging.getLogger(__name__)

# 72h Consumption Profiling Constants
PROFILE_HOURS = 72  # D√©lka profilu v hodin√°ch (3 dny)
# Plovouc√≠ okno: matching + predikce = v≈ædy 72h celkem
# P≈ôed p≈Ølnoc√≠: matching a≈æ do p≈ôedchoz√≠ p≈Ølnoci (max 48h), predikce a≈æ do dal≈°√≠ p≈Ølnoci (min 24h)
# Po p≈Ølnoci: matching jen 24h zpƒõt, predikce 48h dop≈ôedu

# Similarity scoring weights
WEIGHT_CORRELATION = 0.50  # Correlation coefficient weight
WEIGHT_RMSE = 0.30  # RMSE weight (inverted)
WEIGHT_TOTAL = 0.20  # Total consumption difference weight (inverted)


def _get_season(dt: datetime) -> str:
    """Urƒçit roƒçn√≠ obdob√≠ z data."""
    month = dt.month
    if month in [12, 1, 2]:
        return "winter"
    elif month in [3, 4, 5]:
        return "spring"
    elif month in [6, 7, 8]:
        return "summer"
    else:
        return "autumn"


def _generate_profile_name(
    hourly_consumption: List[float], season: str, is_weekend: bool
) -> str:
    """
    Generuje lidsky ƒçiteln√Ω n√°zev profilu na z√°kladƒõ charakteristik spot≈ôeby.

    Args:
        hourly_consumption: 24h profil hodinov√© spot≈ôeby [kWh]
        season: roƒçn√≠ obdob√≠ ('winter', 'spring', 'summer', 'autumn')
        is_weekend: True pokud jde o v√≠kend

    Returns:
        Lidsky ƒçiteln√Ω n√°zev (nap≈ô. "Pracovn√≠ den s topen√≠m", "V√≠kend s pran√≠m")
    """
    if not hourly_consumption or len(hourly_consumption) != 24:
        return "Nezn√°m√Ω profil"

    # 1. Z√ÅKLADN√ç KLASIFIKACE
    day_name = "V√≠kend" if is_weekend else "Pracovn√≠ den"

    # Celkov√° denn√≠ spot≈ôeba
    total = sum(hourly_consumption)
    daily_avg = total / 24

    # 2. ANAL√ùZA PATTERN SHAPE
    morning_avg = float(np.mean(hourly_consumption[6:12]))  # 6-12h
    afternoon_avg = float(np.mean(hourly_consumption[12:18]))  # 12-18h
    evening_avg = float(np.mean(hourly_consumption[18:24]))  # 18-24h
    night_avg = float(np.mean(hourly_consumption[0:6]))  # 0-6h

    # Detekce ≈°piƒçek (≈°piƒçka > 1.3√ó pr≈Ømƒõr)
    has_morning_spike = morning_avg > daily_avg * 1.3
    has_evening_spike = evening_avg > daily_avg * 1.3
    has_afternoon_spike = afternoon_avg > daily_avg * 1.3

    # 3. SPECI√ÅLN√ç DETEKCE
    special_tags = []

    # Topen√≠ (zimn√≠ vysok√° veƒçern√≠ spot≈ôeba)
    if season == "winter" and evening_avg > 1.2:
        special_tags.append("topen√≠")

    # Klimatizace (letn√≠ vysok√° odpoledn√≠ spot≈ôeba)
    if season == "summer" and afternoon_avg > 1.0:
        special_tags.append("klimatizace")

    # Pran√≠ (v√≠kend s rann√≠ ≈°piƒçkou)
    if is_weekend and has_morning_spike:
        special_tags.append("pran√≠")

    # Home office (pracovn√≠ den s vysokou denn√≠ spot≈ôebou)
    if not is_weekend and afternoon_avg > 0.8:
        special_tags.append("home office")

    # Vysok√° noƒçn√≠ spot≈ôeba (bojler?)
    if night_avg > 0.5:
        special_tags.append("noƒçn√≠ oh≈ôev")

    # 4. SESTAVEN√ç N√ÅZVU
    if special_tags:
        # Preferovat speci√°ln√≠ tag
        main_tag = special_tags[0]
        if main_tag == "topen√≠":
            return f"{day_name} s topen√≠m"
        elif main_tag == "klimatizace":
            return f"{day_name} s klimatizac√≠"
        elif main_tag == "pran√≠":
            return f"{day_name} s pran√≠m"
        elif main_tag == "home office":
            return "Home office"
        elif main_tag == "noƒçn√≠ oh≈ôev":
            return f"{day_name} s noƒçn√≠m oh≈ôevem"

    # Fallback podle ≈°piƒçky
    if has_evening_spike:
        return f"{day_name} - veƒçern√≠ ≈°piƒçka"
    elif has_morning_spike:
        return f"{day_name} - rann√≠ ≈°piƒçka"
    elif has_afternoon_spike:
        return f"{day_name} - poledn√≠ ≈°piƒçka"
    else:
        return f"{day_name} - bƒõ≈æn√Ω"


class OigCloudAdaptiveLoadProfilesSensor(CoordinatorEntity, SensorEntity):
    """
    Sensor pro automatickou anal√Ωzu a tvorbu profil≈Ø spot≈ôeby.

    - Noƒçn√≠ anal√Ωza historick√Ωch dat (02:00)
    - Persistence profil≈Ø v attributes
    - UI-friendly zobrazen√≠
    """

    def __init__(
        self,
        coordinator: Any,
        sensor_type: str,
        config_entry: ConfigEntry,
        device_info: Dict[str, Any],
        hass: Optional[HomeAssistant] = None,
    ) -> None:
        """Initialize the adaptive profiles sensor."""
        super().__init__(coordinator)

        self._sensor_type = sensor_type
        self._config_entry = config_entry
        self._device_info = device_info
        self._hass: Optional[HomeAssistant] = hass or getattr(coordinator, "hass", None)

        # Stabiln√≠ box_id resolution (config entry ‚Üí proxy ‚Üí coordinator numeric keys)
        try:
            from .oig_cloud_sensor import resolve_box_id

            self._box_id = resolve_box_id(coordinator)
        except Exception:
            self._box_id = "unknown"

        self._attr_unique_id = f"oig_cloud_{self._box_id}_{sensor_type}"
        self.entity_id = f"sensor.oig_{self._box_id}_{sensor_type}"
        self._attr_icon = "mdi:chart-timeline-variant-shimmer"
        self._attr_native_unit_of_measurement = None  # State = poƒçet profil≈Ø
        self._attr_device_class = None
        self._attr_state_class = None
        self._attr_entity_category = EntityCategory.DIAGNOSTIC

        # Naƒç√≠st n√°zev ze sensor types
        from .sensors.SENSOR_TYPES_STATISTICS import SENSOR_TYPES_STATISTICS

        sensor_config = SENSOR_TYPES_STATISTICS.get(sensor_type, {})
        name_cs = sensor_config.get("name_cs")
        name_en = sensor_config.get("name")
        self._attr_name = name_cs or name_en or sensor_type

        # 72h Profiling storage
        self._last_profile_created: Optional[datetime] = None
        self._profiling_status: str = "idle"  # idle/creating/ok/error
        self._profiling_error: Optional[str] = None
        self._profiling_task: Optional[Any] = None  # Background task

        # Current consumption prediction (from coordinator)
        self._current_prediction: Optional[Dict[str, Any]] = None

    async def async_added_to_hass(self) -> None:
        """P≈ôi p≈ôid√°n√≠ do HA - spustit profiling loop."""
        await super().async_added_to_hass()
        self._hass = self.hass

        # START: Profiling loop jako background task
        _LOGGER.info("Starting consumption profiling loop")
        self._profiling_task = self.hass.async_create_background_task(
            self._profiling_loop(), name="oig_cloud_consumption_profiling_loop"
        )

    async def async_will_remove_from_hass(self) -> None:
        """P≈ôi odebr√°n√≠ z HA - zru≈°it profiling task."""
        if self._profiling_task and not self._profiling_task.done():
            self._profiling_task.cancel()
        await super().async_will_remove_from_hass()

    async def _profiling_loop(self) -> None:
        """
        Profiling loop - vytv√°≈ôen√≠ adaptivn√≠ predikce spot≈ôeby.

        Prvn√≠ bƒõh okam≈æitƒõ (s delay 10s), pak ka≈æd√Ωch 15 minut.
        Historick√© profily se loaduj√≠ jednou dennƒõ v 00:30.
        """
        try:
            # Prvn√≠ bƒõh s delay aby HA dostal ƒças
            await asyncio.sleep(10)

            _LOGGER.info(
                "üìä Adaptive profiling loop starting - matching every 15 minutes"
            )

            # Prvn√≠ bƒõh okam≈æitƒõ
            await self._create_and_update_profile()

            while True:
                try:
                    # ƒåekat 15 minut
                    await asyncio.sleep(15 * 60)

                    _LOGGER.debug("üìä Running adaptive matching (15min update)")
                    await self._create_and_update_profile()

                except Exception as e:
                    _LOGGER.error(f"‚ùå Profiling loop error: {e}", exc_info=True)
                    self._profiling_status = "error"
                    self._profiling_error = str(e)
                    self.async_schedule_update_ha_state(force_refresh=True)

                    # Poƒçkat 5 minut p≈ôed retry po chybƒõ
                    await asyncio.sleep(5 * 60)

        except asyncio.CancelledError:
            _LOGGER.info("Profiling loop cancelled")
            raise
        except Exception as e:
            _LOGGER.error(f"Fatal profiling loop error: {e}", exc_info=True)

    async def _wait_for_next_profile_window(self) -> None:
        """Poƒçkat do dal≈°√≠ho profiling okna (00:30)."""
        now = dt_util.now()
        target_time = now.replace(hour=0, minute=30, second=0, microsecond=0)

        # Pokud u≈æ je po 00:30 dnes, ƒçekat na z√≠tra
        if now >= target_time:
            target_time += timedelta(days=1)

        wait_seconds = (target_time - now).total_seconds()
        _LOGGER.info(
            f"‚è±Ô∏è Waiting {wait_seconds / 3600:.1f} hours until next profile window at {target_time}"
        )

        await asyncio.sleep(wait_seconds)

    async def _create_and_update_profile(self) -> None:
        """Vytvo≈ôit profil a updateovat state."""
        self._profiling_status = "creating"
        self._profiling_error = None
        if self._hass:
            self.async_write_ha_state()

        consumption_sensor = f"sensor.oig_{self._box_id}_actual_aco_p"

        # Naj√≠t best matching profile p≈ô√≠mo z aktu√°ln√≠ch dat
        # (nepot≈ôebujeme ukl√°dat do events - profily jsou on-the-fly)
        prediction = await self._find_best_matching_profile(consumption_sensor)

        if prediction:
            self._last_profile_created = dt_util.now()
            self._profiling_status = "ok"
            self._profiling_error = None
            self._current_prediction = prediction

            _LOGGER.info(
                f"‚úÖ Profile updated: predicted {prediction.get('predicted_total_kwh', 0):.2f} kWh for next 24h"
            )
        else:
            self._profiling_status = "error"
            self._profiling_error = "Failed to create profile"
            _LOGGER.warning("‚ùå Failed to update consumption profile")

        if self._hass:
            self.async_write_ha_state()

            # Notify dependent sensors (BatteryForecast) that profiles are ready
            if prediction:  # Only signal if we have valid data
                from homeassistant.helpers.dispatcher import async_dispatcher_send

                signal_name = f"oig_cloud_{self._box_id}_profiles_updated"
                _LOGGER.debug(f"üì° Sending signal: {signal_name}")
                async_dispatcher_send(self._hass, signal_name)

    # ============================================================================
    # 72h Consumption Profiling System
    # ============================================================================

    async def _get_consumption_history_72h(
        self, consumption_sensor_entity_id: str
    ) -> Optional[List[float]]:
        """
        Naƒç√≠st 72 hodin spot≈ôeby ze statistics tabulky (hourly pr≈Ømƒõry).

        Args:
            consumption_sensor_entity_id: Entity ID senzoru spot≈ôeby

        Returns:
            List 72 float hodnot (hodinov√© pr≈Ømƒõry v kWh), nebo None p≈ôi chybƒõ
        """
        if not self._hass:
            _LOGGER.warning("Cannot get consumption history - no hass instance")
            return None

        try:
            from homeassistant.helpers.recorder import get_instance
            from sqlalchemy import text

            recorder_instance = get_instance(self._hass)
            if not recorder_instance:
                _LOGGER.error("Recorder instance not available")
                return None

            end_time = dt_util.now()
            start_time = end_time - timedelta(hours=PROFILE_HOURS)

            # SQL query pro statistics tabulku
            def get_hourly_statistics():
                """Query statistics table for hourly averages."""
                from homeassistant.helpers.recorder import session_scope

                instance = get_instance(self._hass)
                with session_scope(
                    hass=self._hass, session=instance.get_session()
                ) as session:
                    start_ts = int(start_time.timestamp())
                    end_ts = int(end_time.timestamp())

                    query = text(
                        """
                        SELECT s.mean, s.start_ts
                        FROM statistics s
                        INNER JOIN statistics_meta sm ON s.metadata_id = sm.id
                        WHERE sm.statistic_id = :statistic_id
                        AND s.start_ts >= :start_ts
                        AND s.start_ts < :end_ts
                        AND s.mean IS NOT NULL
                        ORDER BY s.start_ts
                        """
                    )

                    result = session.execute(
                        query,
                        {
                            "statistic_id": consumption_sensor_entity_id,
                            "start_ts": start_ts,
                            "end_ts": end_ts,
                        },
                    )
                    return result.fetchall()

            # Execute query in executor
            _LOGGER.debug(
                f"Loading 72h statistics for {consumption_sensor_entity_id}..."
            )
            stats_rows = await self._hass.async_add_executor_job(get_hourly_statistics)

            if not stats_rows:
                _LOGGER.warning(
                    f"No statistics data for {consumption_sensor_entity_id}"
                )
                return None

            # Convert to hourly consumption list
            hourly_data = {}
            for row in stats_rows:
                try:
                    mean_value = float(row[0])  # mean power in W
                    timestamp_ts = float(row[1])  # UNIX timestamp

                    # Convert to datetime
                    timestamp = datetime.fromtimestamp(timestamp_ts, tz=dt_util.UTC)

                    # Sanity check
                    if mean_value < 0 or mean_value > 20000:
                        continue

                    # Calculate hour offset from start
                    hour_offset = int((timestamp - start_time).total_seconds() / 3600)
                    if 0 <= hour_offset < PROFILE_HOURS:
                        # W ‚Üí kWh (hourly average)
                        hourly_data[hour_offset] = mean_value / 1000.0

                except (ValueError, AttributeError, IndexError):
                    continue

            # Build final list with 0 for missing hours
            hourly_consumption = []
            for hour_offset in range(PROFILE_HOURS):
                if hour_offset in hourly_data:
                    hourly_consumption.append(hourly_data[hour_offset])
                else:
                    hourly_consumption.append(0.0)

            if len(hourly_consumption) != PROFILE_HOURS:
                _LOGGER.warning(
                    f"Expected {PROFILE_HOURS} hours, got {len(hourly_consumption)}"
                )
                return None

            total_kwh = sum(hourly_consumption)
            _LOGGER.info(
                f"‚úÖ Loaded 72h statistics: {len(stats_rows)} records, total {total_kwh:.2f} kWh"
            )

            return hourly_consumption

        except Exception as e:
            _LOGGER.error(f"Failed to get consumption history: {e}", exc_info=True)
            return None

    async def _load_historical_profiles(
        self, consumption_sensor_entity_id: str, days_back: int = 90
    ) -> List[Dict[str, Any]]:
        """
        Naƒç√≠st historick√© 72h profily ze statistics (sliding window po dnech).

        Args:
            consumption_sensor_entity_id: Entity ID senzoru spot≈ôeby
            days_back: Kolik dn√≠ zpƒõt naƒç√≠st (default 90 = ~3 mƒõs√≠ce)

        Returns:
            List profil≈Ø, ka≈æd√Ω profil = 72h consumption data
        """
        if not self._hass:
            return []

        try:
            from homeassistant.helpers.recorder import get_instance
            from sqlalchemy import text

            recorder_instance = get_instance(self._hass)
            if not recorder_instance:
                _LOGGER.error("Recorder instance not available")
                return []

            end_time = dt_util.now()
            start_time = end_time - timedelta(days=days_back)

            # Naƒç√≠st v≈°echna dostupn√° data z statistics tabulky
            def get_all_statistics():
                """Query statistics for all hourly averages."""
                from homeassistant.helpers.recorder import session_scope

                instance = get_instance(self._hass)
                with session_scope(
                    hass=self._hass, session=instance.get_session()
                ) as session:
                    start_ts = int(start_time.timestamp())
                    end_ts = int(end_time.timestamp())

                    query = text(
                        """
                        SELECT s.mean, s.start_ts
                        FROM statistics s
                        INNER JOIN statistics_meta sm ON s.metadata_id = sm.id
                        WHERE sm.statistic_id = :statistic_id
                        AND s.start_ts >= :start_ts
                        AND s.start_ts < :end_ts
                        AND s.mean IS NOT NULL
                        ORDER BY s.start_ts
                        """
                    )

                    result = session.execute(
                        query,
                        {
                            "statistic_id": consumption_sensor_entity_id,
                            "start_ts": start_ts,
                            "end_ts": end_ts,
                        },
                    )
                    return result.fetchall()

            # Execute query
            _LOGGER.debug(
                f"Loading historical statistics for profile matching ({days_back} days)..."
            )
            stats_rows = await self._hass.async_add_executor_job(get_all_statistics)

            if not stats_rows:
                _LOGGER.warning(
                    f"No historical statistics data for {consumption_sensor_entity_id}"
                )
                return []

            # Convert to hourly consumption array
            hourly_data = []
            for row in stats_rows:
                try:
                    mean_value = float(row[0])  # mean power in W

                    # Sanity check
                    if mean_value < 0 or mean_value > 20000:
                        continue

                    # W ‚Üí kWh
                    hourly_data.append(mean_value / 1000.0)

                except (ValueError, AttributeError, IndexError):
                    continue

            if len(hourly_data) < PROFILE_HOURS:
                _LOGGER.warning(f"Not enough historical data: {len(hourly_data)} hours")
                return []

            # Create sliding window profiles (ka≈æd√Ω den = nov√Ω profil)
            # Posuneme o 24h, tak≈æe ka≈æd√Ω profil je jin√Ω den+p≈ôedchoz√≠ch 48h
            profiles = []
            step = 24  # Posun po 24h (1 den)

            for i in range(0, len(hourly_data) - PROFILE_HOURS + 1, step):
                profile_data = hourly_data[i : i + PROFILE_HOURS]

                if len(profile_data) == PROFILE_HOURS:
                    profiles.append(
                        {
                            "consumption_kwh": profile_data,
                            "total_consumption": float(np.sum(profile_data)),
                            "avg_consumption": float(np.mean(profile_data)),
                        }
                    )

            _LOGGER.info(
                f"‚úÖ Loaded {len(profiles)} historical 72h profiles from {len(hourly_data)} hours of data"
            )

            return profiles

        except Exception as e:
            _LOGGER.error(f"Failed to load historical profiles: {e}", exc_info=True)
            return []

    def _calculate_profile_similarity(
        self, current_data: List[float], profile_data: List[float]
    ) -> float:
        """
        Spoƒç√≠tat similarity score mezi aktu√°ln√≠mi daty a historick√Ωm profilem.

        Scoring:
        - 50% correlation coefficient (Pearson≈Øv korelaƒçn√≠ koeficient)
        - 30% RMSE (root mean square error - inverted)
        - 20% total consumption difference (inverted)

        Args:
            current_data: Aktu√°ln√≠ spot≈ôeba (plovouc√≠ poƒçet hodin)
            profile_data: Historick√Ω profil (stejn√Ω poƒçet hodin)

        Returns:
            Similarity score 0.0 - 1.0 (1.0 = perfektn√≠ match)
        """
        if len(current_data) != len(profile_data):
            _LOGGER.warning(
                f"Invalid data length for similarity: {len(current_data)} != {len(profile_data)}"
            )
            return 0.0

        try:
            # Convert to numpy arrays
            current = np.array(current_data)
            profile = np.array(profile_data)

            # 1. Correlation coefficient (50%)
            correlation = np.corrcoef(current, profile)[0, 1]
            # Normalize to 0-1 (correlation je -1 a≈æ 1, chceme jen pozitivn√≠ podobnost)
            correlation_score = max(0.0, correlation)

            # 2. RMSE (30%) - lower is better, normalize to 0-1
            rmse = np.sqrt(np.mean((current - profile) ** 2))
            # Normalize: exponenci√°ln√≠ decay, RMSE=0 ‚Üí score=1, RMSE roste ‚Üí score kles√°
            max_reasonable_rmse = 5.0  # kWh
            rmse_score = np.exp(-rmse / max_reasonable_rmse)

            # 3. Total consumption difference (20%) - lower is better
            total_current = np.sum(current)
            total_profile = np.sum(profile)
            if total_profile > 0:
                total_diff = abs(total_current - total_profile) / total_profile
            else:
                total_diff = 1.0 if total_current > 0 else 0.0

            # Normalize: 0% diff ‚Üí score=1, 100%+ diff ‚Üí score‚âà0
            total_score = np.exp(-total_diff)

            # Weighted sum
            similarity = (
                WEIGHT_CORRELATION * correlation_score
                + WEIGHT_RMSE * rmse_score
                + WEIGHT_TOTAL * total_score
            )

            return float(similarity)

        except Exception as e:
            _LOGGER.error(f"Failed to calculate similarity: {e}", exc_info=True)
            return 0.0

    async def _find_best_matching_profile(
        self, current_consumption_sensor: str
    ) -> Optional[Dict[str, Any]]:
        """
        Naj√≠t nejlep≈°√≠ matching 72h profil pro aktu√°ln√≠ spot≈ôebu.

        Plovouc√≠ okno:
        - P≈ôed p≈Ølnoc√≠ (nap≈ô. 20:00): matching 44h zpƒõt, predikce 28h dop≈ôedu
        - Po p≈Ølnoci (nap≈ô. 01:00): matching 24h zpƒõt, predikce 48h dop≈ôedu
        - V≈ædy celkem 72h

        Args:
            current_consumption_sensor: Entity ID senzoru spot≈ôeby

        Returns:
            Best matching profil s predicted consumption, nebo None
        """
        if not self._hass:
            return None

        try:
            # Spoƒç√≠tat plovouc√≠ okno
            now = dt_util.now()
            current_hour = now.hour

            # Kolik hodin uplynulo od p≈Ølnoci (0-23)
            hours_since_midnight = current_hour

            # Matching: od p≈ôedchoz√≠ p≈Ølnoci do teƒè
            # - P≈ôed p≈Ølnoc√≠: m≈Ø≈æe b√Ωt a≈æ 48h (cel√Ω vƒçerej≈°ek + ƒç√°st dne≈°ka)
            # - Po p≈Ølnoci: maxim√°lnƒõ 24h (jen dne≈°ek)
            match_hours = hours_since_midnight + 24 if hours_since_midnight > 0 else 24

            # Predikce: zbytek do 72h
            predict_hours = PROFILE_HOURS - match_hours

            _LOGGER.debug(
                f"Plovouc√≠ okno: ƒças={current_hour}:00, "
                f"matching={match_hours}h zpƒõt, predikce={predict_hours}h dop≈ôedu"
            )

            # 1. Naƒç√≠st aktu√°ln√≠ 72h spot≈ôeby
            current_72h = await self._get_consumption_history_72h(
                current_consumption_sensor
            )

            if not current_72h or len(current_72h) < match_hours:
                _LOGGER.warning(
                    f"Not enough current consumption data for matching "
                    f"(need {match_hours}h, got {len(current_72h) if current_72h else 0}h)"
                )
                return None

            # Vezmi posledn√≠ch N hodin pro matching
            current_match = current_72h[-match_hours:]

            # 2. Naƒç√≠st historick√© profily
            profiles = await self._load_historical_profiles(current_consumption_sensor)

            if not profiles:
                _LOGGER.warning("No historical profiles available for matching")
                return None

            # 3. Naj√≠t best match (porovn√°v√°me prvn√≠ch match_hours ka≈æd√©ho profilu)
            best_match = None
            best_score = 0.0

            for profile in profiles:
                profile_data = profile.get("consumption_kwh", [])
                if len(profile_data) != PROFILE_HOURS:
                    continue

                # Vezmi prvn√≠ch match_hours z profilu
                profile_match = profile_data[:match_hours]

                # Spoƒç√≠tat similarity
                score = self._calculate_profile_similarity(current_match, profile_match)

                if score > best_score:
                    best_score = score
                    best_match = profile

            if not best_match:
                _LOGGER.warning("No matching profile found")
                return None

            # 4. Extrahovat predikci (hodin match_hours a≈æ match_hours+predict_hours)
            matched_consumption = best_match.get("consumption_kwh", [])
            predicted = matched_consumption[match_hours : match_hours + predict_hours]

            result = {
                "matched_profile_created": best_match.get("created_at"),
                "similarity_score": best_score,
                "predicted_consumption": predicted,
                "predicted_total_kwh": float(np.sum(predicted)),
                "predicted_avg_kwh": float(np.mean(predicted)),
                "matched_profile_total": best_match.get("total_consumption"),
                "matched_profile_full": matched_consumption,  # Cel√Ω 72h profil pro generov√°n√≠ n√°zvu
                "match_hours": match_hours,
                "predict_hours": predict_hours,
            }

            _LOGGER.info(
                f"üéØ Best matching profile: score={best_score:.3f}, "
                f"predicted_{predict_hours}h={result['predicted_total_kwh']:.2f} kWh"
            )

            return result

        except Exception as e:
            _LOGGER.error(f"Failed to find matching profile: {e}", exc_info=True)
            return None

    @property
    def native_value(self) -> Optional[str]:
        """Return profiling status."""
        if self._current_prediction:
            total = self._current_prediction.get("predicted_total_kwh", 0)
            return f"{total:.1f} kWh"
        return "no_data"

    @property
    def extra_state_attributes(self) -> Dict[str, Any]:
        """Return attributes."""
        attrs = {
            "profiling_status": self._profiling_status,
            "profiling_error": self._profiling_error,
            "last_profile_created": (
                self._last_profile_created.isoformat()
                if self._last_profile_created
                else None
            ),
        }

        # Add prediction summary if available
        if self._current_prediction:
            attrs["prediction_summary"] = {
                "similarity_score": self._current_prediction.get("similarity_score"),
                "predicted_total_kwh": self._current_prediction.get(
                    "predicted_total_kwh"
                ),
                "predicted_avg_kwh": self._current_prediction.get("predicted_avg_kwh"),
            }

            # Add today_profile and tomorrow_profile for battery_forecast integration
            predicted = self._current_prediction.get("predicted_consumption", [])
            predict_hours = self._current_prediction.get("predict_hours", 0)

            if predicted and predict_hours > 0:
                similarity_score = self._current_prediction.get("similarity_score", 0)
                now = dt_util.now()
                current_hour = now.hour

                # Kolik hodin zb√Ωv√° do p≈Ølnoci (vƒçetnƒõ aktu√°ln√≠ hodiny)
                hours_until_midnight = 24 - current_hour

                # TODAY: Zb√Ωvaj√≠c√≠ ƒç√°st dne≈°ka (od current_hour do p≈Ølnoci)
                # - Vezmi prvn√≠ch min(hours_until_midnight, predict_hours) hodin z predikce
                today_count = min(hours_until_midnight, predict_hours)
                today_hours = predicted[:today_count]

                # TOMORROW: Zbytek predikce (od p≈Ølnoci)
                tomorrow_hours = (
                    predicted[today_count:] if today_count < predict_hours else []
                )

                # Doplnƒõn√≠ tomorrow na 24h pokud je krat≈°√≠ (padding s pr≈Ømƒõrem)
                if len(tomorrow_hours) < 24:
                    avg_hour = (
                        float(np.mean(tomorrow_hours))
                        if len(tomorrow_hours) > 0
                        else 0.5
                    )
                    tomorrow_hours = list(tomorrow_hours) + [avg_hour] * (
                        24 - len(tomorrow_hours)
                    )

                # Vytvo≈ô metadata
                season = _get_season(now)
                is_weekend_today = now.weekday() >= 5
                is_weekend_tomorrow = (now.weekday() + 1) % 7 >= 5

                # Vygenerovat n√°zvy z matched profilu (72h)
                # Pro dne≈°ek: pou≈æ√≠t zbytek dne≈°n√≠ho dne z matched profilu
                # Pro z√≠t≈ôek: pou≈æ√≠t cel√Ω z√≠t≈ôej≈°√≠ den z matched profilu
                matched_profile_full = self._current_prediction.get(
                    "matched_profile_full", []
                )

                if len(matched_profile_full) >= 72:
                    # Matched profil: [vƒçera 24h | dnes 24h | z√≠tra 24h]
                    # Index 48-71 = z√≠t≈ôek (posledn√≠ 24h)
                    tomorrow_from_matched = matched_profile_full[48:72]

                    # Pro dne≈°ek: vezmi aktu√°ln√≠ hodinu a≈æ konec dne z matched profilu
                    # Matched profil konƒç√≠ "z√≠tra 24:00", tak≈æe "dnes" je hodiny 24-47
                    today_start_in_matched = (
                        24 + current_hour
                    )  # nap≈ô. 24+14=38 pro 14:00
                    today_from_matched = matched_profile_full[today_start_in_matched:48]
                else:
                    # Fallback pokud matched profil nen√≠ dostupn√Ω
                    tomorrow_from_matched = tomorrow_hours[:24]
                    today_from_matched = today_hours

                # Generovat n√°zvy z odpov√≠daj√≠c√≠ch ƒç√°st√≠ matched profilu
                today_profile_name = _generate_profile_name(
                    hourly_consumption=(
                        today_from_matched
                        if len(today_from_matched) == 24
                        else (
                            today_from_matched + [0.0] * (24 - len(today_from_matched))
                        )
                    ),
                    season=season,
                    is_weekend=is_weekend_today,
                )

                tomorrow_profile_name = _generate_profile_name(
                    hourly_consumption=tomorrow_from_matched,
                    season=season,
                    is_weekend=is_weekend_tomorrow,
                )

                today_profile_data = {
                    "hourly_consumption": today_hours,
                    "start_hour": current_hour,  # Hodina od kter√© zaƒç√≠n√° pole (14 = index 0 je 14:00)
                    "total_kwh": float(np.sum(today_hours)),
                    "avg_kwh_h": (
                        float(np.mean(today_hours)) if len(today_hours) > 0 else 0.0
                    ),
                    "ui": {
                        "name": today_profile_name,
                        "similarity_score": similarity_score,
                    },
                    "characteristics": {
                        "season": season,
                        "is_weekend": is_weekend_today,
                    },
                    "sample_count": 1,
                }

                # TOMORROW profile (n√°zev u≈æ vygenerovan√Ω naho≈ôe)
                tomorrow_profile_data = {
                    "hourly_consumption": tomorrow_hours[:24],
                    "start_hour": 0,  # Z√≠t≈ôek v≈ædy zaƒç√≠n√° od p≈Ølnoci (0:00)
                    "total_kwh": float(np.sum(tomorrow_hours[:24])),
                    "avg_kwh_h": float(np.mean(tomorrow_hours[:24])),
                    "ui": {
                        "name": tomorrow_profile_name,
                        "similarity_score": similarity_score,
                    },
                    "characteristics": {
                        "season": season,
                        "is_weekend": is_weekend_tomorrow,
                    },
                    "sample_count": 1,
                }

                attrs["today_profile"] = today_profile_data
                attrs["tomorrow_profile"] = tomorrow_profile_data

        return attrs

    def get_current_prediction(self) -> Optional[Dict[str, Any]]:
        """Get current consumption prediction for use by other components."""
        return self._current_prediction

    @property
    def device_info(self) -> Dict[str, Any]:
        """Return device info."""
        return self._device_info
