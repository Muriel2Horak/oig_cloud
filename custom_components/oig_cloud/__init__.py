"""The OIG Cloud integration."""

from __future__ import annotations

import logging
import hashlib
import re
from typing import Any, Dict

from homeassistant import config_entries, core
from homeassistant.config_entries import ConfigEntry
from homeassistant.const import Platform
from homeassistant.core import HomeAssistant
from homeassistant.exceptions import ConfigEntryNotReady

from .lib.oig_cloud_client.api.oig_cloud_api import OigCloudApi
from .const import (
    CONF_NO_TELEMETRY,
    CONF_USERNAME,
    CONF_PASSWORD,
    DOMAIN,
    CONF_STANDARD_SCAN_INTERVAL,
    CONF_EXTENDED_SCAN_INTERVAL,
    CONF_AUTO_MODE_SWITCH,
    CONF_AUTO_MODE_PLAN,
)
from .oig_cloud_coordinator import OigCloudCoordinator
from .data_source import (
    DataSourceController,
    DATA_SOURCE_CLOUD_ONLY,
    DEFAULT_DATA_SOURCE_MODE,
    DEFAULT_PROXY_STALE_MINUTES,
    DEFAULT_LOCAL_EVENT_DEBOUNCE_MS,
    get_data_source_state,
    init_data_source_state,
)

# OPRAVA: BezpeÄnÃ½ import BalancingManager s try/except
try:
    from .balancing import BalancingManager

    _LOGGER_TEMP = logging.getLogger(__name__)
    _LOGGER_TEMP.debug("oig_cloud: BalancingManager import OK")
except Exception as err:
    BalancingManager = None
    _LOGGER_TEMP = logging.getLogger(__name__)
    _LOGGER_TEMP.error(
        "oig_cloud: Failed to import BalancingManager: %s", err, exc_info=True
    )

_LOGGER = logging.getLogger(__name__)

PLATFORMS: list[Platform] = [Platform.SENSOR]

# **OPRAVA: GlobÃ¡lnÃ­ analytics_device_info pro statistickÃ© senzory**
analytics_device_info: Dict[str, Any] = {
    "identifiers": {(DOMAIN, "analytics")},
    "name": "Analytics & Predictions",
    "manufacturer": "OIG Cloud",
    "model": "Analytics Module",
    "sw_version": "1.0",
}

# OPRAVA: Definujeme vÅ¡echny moÅ¾nÃ© box modes pro konzistenci
ALL_BOX_MODES = ["Home 1", "Home 2", "Home 3", "Home UPS", "Home 5", "Home 6"]


def _ensure_data_source_option_defaults(hass: HomeAssistant, entry: ConfigEntry) -> None:
    defaults = {
        "data_source_mode": DEFAULT_DATA_SOURCE_MODE,
        "local_proxy_stale_minutes": DEFAULT_PROXY_STALE_MINUTES,
        "local_event_debounce_ms": DEFAULT_LOCAL_EVENT_DEBOUNCE_MS,
    }

    options = dict(entry.options)
    updated = False
    for key, default in defaults.items():
        if options.get(key) is None:
            options[key] = default
            updated = True

    if updated:
        hass.config_entries.async_update_entry(entry, options=options)


def _infer_box_id_from_local_entities(hass: HomeAssistant) -> str | None:
    """Best-effort inference of box_id from existing oig_local entity_ids.

    Expected local entity_id pattern: sensor.oig_local_<box_id>_<suffix>
    """
    try:
        from homeassistant.helpers import entity_registry as er

        reg = er.async_get(hass)
        ids: set[str] = set()
        pat = re.compile(r"^sensor\\.oig_local_(\\d+)_")
        for ent in reg.entities.values():
            m = pat.match(ent.entity_id)
            if m:
                ids.add(m.group(1))
        if len(ids) == 1:
            return next(iter(ids))
        return None
    except Exception:
        return None


def _ensure_planner_option_defaults(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """Ensure new planner-related options exist on legacy config entries."""

    defaults = {
        "battery_planner_mode": "hybrid_autonomy",
        "enable_autonomous_preview": True,
        CONF_AUTO_MODE_SWITCH: False,
        CONF_AUTO_MODE_PLAN: "autonomy",
        "enable_cheap_window_ups": True,
        "cheap_window_percentile": 30,
        "cheap_window_max_intervals": 20,
        "cheap_window_soc_guard_kwh": 0.5,
        "autonomy_soc_step_kwh": 0.5,
        "autonomy_target_penalty": 0.5,
        "autonomy_min_penalty": 2.0,
        "autonomy_negative_export_penalty": 50.0,
    }

    options = dict(entry.options)
    missing_keys = [key for key in defaults.keys() if entry.options.get(key) is None]
    updated = False

    for key, default in defaults.items():
        if options.get(key) is None:
            options[key] = default
            updated = True

    # Migrace starÅ¡Ã­ch hodnot na novÃ© vyvÃ¡Å¾enÃ© defaulty
    if options.get("autonomy_min_penalty") in (15.0, 8.0):
        options["autonomy_min_penalty"] = defaults["autonomy_min_penalty"]
        updated = True
    if options.get("autonomy_target_penalty") == 3.0:
        options["autonomy_target_penalty"] = defaults["autonomy_target_penalty"]
        updated = True
    if updated:
        _LOGGER.info(
            "ðŸ”§ Injecting missing planner options for entry %s: %s",
            entry.entry_id,
            ", ".join(missing_keys) if missing_keys else "none",
        )
        hass.config_entries.async_update_entry(entry, options=options)


async def async_setup(hass: HomeAssistant, config: Dict[str, Any]) -> bool:
    """Set up OIG Cloud integration."""
    # OPRAVA: Debug setup telemetrie
    print("[OIG SETUP] Starting OIG Cloud setup")

    # OPRAVA: OdstranÃ­me neexistujÃ­cÃ­ import setup_telemetry
    # Initialize telemetry - telemetrie se inicializuje pÅ™Ã­mo v ServiceShield
    print("[OIG SETUP] Telemetry will be initialized in ServiceShield")

    # OPRAVA: ServiceShield se inicializuje pouze v async_setup_entry, ne zde
    # V async_setup pouze pÅ™ipravÃ­me globÃ¡lnÃ­ strukturu
    hass.data.setdefault(DOMAIN, {})
    print("[OIG SETUP] Global data structure prepared")

    # OPRAVA: UniverzÃ¡lnÃ­ registrace statickÃ½ch cest pro vÅ¡echny verze HA
    await _register_static_paths(hass)

    # OPRAVA: OdstranÄ›nÃ­ volÃ¡nÃ­ _setup_frontend_panel z async_setup
    # Panel se registruje aÅ¾ v async_setup_entry kde mÃ¡me pÅ™Ã­stup k entry
    # await _setup_frontend_panel(hass)  # ODSTRANÄšNO

    print("[OIG SETUP] OIG Cloud setup completed")
    return True


async def _register_static_paths(hass: HomeAssistant) -> None:
    """Registrace statickÃ½ch cest pro HA 2024.5+."""
    static_path = "/oig_cloud_static"
    directory = hass.config.path("custom_components/oig_cloud/www")

    _LOGGER.info("Registering static path: %s -> {directory}", static_path)

    # OPRAVA: Pouze modernÃ­ metoda
    from homeassistant.components.http import StaticPathConfig

    static_config = StaticPathConfig(static_path, directory, cache_headers=False)
    await hass.http.async_register_static_paths([static_config])
    _LOGGER.info("âœ… Static paths registered successfully")


async def _setup_frontend_panel(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """NastavenÃ­ frontend panelu (pouze kdyÅ¾ je povolen)."""
    try:
        # UnikÃ¡tnÃ­ ID panelu pro tuto instanci
        panel_id = f"oig_cloud_dashboard_{entry.entry_id}"

        # OPRAVA: ZÃ­skÃ¡nÃ­ inverter_sn pÅ™Ã­mo z coordinator.data
        inverter_sn = "unknown"
        coordinator_data = hass.data[DOMAIN][entry.entry_id].get("coordinator")
        if coordinator_data and coordinator_data.data:
            inverter_sn = next(iter(coordinator_data.data.keys()), "unknown")
            _LOGGER.info("Dashboard setup: Found inverter_sn = %s", inverter_sn)
        else:
            _LOGGER.warning("Dashboard setup: No coordinator data available")

        panel_title = (
            f"OIG Dashboard ({inverter_sn})"
            if inverter_sn != "unknown"
            else "OIG Cloud Dashboard"
        )

        # Cache-busting: PÅ™idat verzi + timestamp k URL pro vymazÃ¡nÃ­ browseru cache
        import os
        import json
        import time

        manifest_path = os.path.join(os.path.dirname(__file__), "manifest.json")
        version = "unknown"
        try:
            # OPRAVA: PouÅ¾Ã­t async file read mÃ­sto blocking open()
            manifest_data = await hass.async_add_executor_job(
                lambda: open(manifest_path, "r").read()
            )
            manifest = json.loads(manifest_data)
            version = manifest.get("version", "unknown")
        except Exception as e:
            _LOGGER.warning("Could not load version from manifest: %s", e)

        # PÅ™idat timestamp pro cache-busting pÅ™i kaÅ¾dÃ©m restartu
        cache_bust = int(time.time())

        # OPRAVA: PÅ™idat parametry vÄetnÄ› v= a t= pro cache-busting
        dashboard_url = f"/oig_cloud_static/dashboard.html?entry_id={entry.entry_id}&inverter_sn={inverter_sn}&v={version}&t={cache_bust}"

        _LOGGER.info("Dashboard URL: %s", dashboard_url)

        from homeassistant.components import frontend

        # OPRAVA: Kontrola existence funkce a jejÃ­ volÃ¡nÃ­ bez await pokud vracÃ­ None
        if hasattr(frontend, "async_register_built_in_panel"):
            register_func = getattr(frontend, "async_register_built_in_panel")
            if callable(register_func):
                try:
                    # OPRAVA: PouÅ¾Ã­t keyword argumenty pro sprÃ¡vnÃ© volÃ¡nÃ­
                    # Signature: (hass, component_name, sidebar_title, sidebar_icon,
                    #             sidebar_default_visible, frontend_url_path, config, require_admin)
                    result = register_func(
                        hass,
                        "iframe",  # component_name
                        sidebar_title=panel_title,
                        sidebar_icon="mdi:solar-power",
                        frontend_url_path=panel_id,
                        config={"url": dashboard_url},
                        require_admin=False,
                    )

                    # Pokud funkce vracÃ­ coroutine, await it
                    if hasattr(result, "__await__"):
                        await result

                    _LOGGER.info("âœ… Panel '%s' registered successfully", panel_title)
                except Exception as reg_error:
                    _LOGGER.error("Error during panel registration: %s", reg_error)
                    raise
            else:
                _LOGGER.warning("async_register_built_in_panel is not callable")
        else:
            _LOGGER.warning("Frontend async_register_built_in_panel not available")

        # OPRAVA: Debug logovÃ¡nÃ­ dostupnÃ½ch entit
        coordinator = hass.data[DOMAIN][entry.entry_id].get("coordinator")
        if coordinator and coordinator.data:
            entity_count = len(
                [
                    k
                    for k in hass.states.async_entity_ids()
                    if k.startswith(f"sensor.oig_{inverter_sn}")
                ]
            )
            _LOGGER.info(
                f"Dashboard: Found {entity_count} OIG entities for inverter {inverter_sn}"
            )

            # Kontrola klÃ­ÄovÃ½ch entit (pouze pokud jsou zapnutÃ© v konfiguraci)
            key_entities = [
                f"sensor.oig_{inverter_sn}_remaining_usable_capacity",  # vÅ¾dy
            ]

            # PÅ™idat solar_forecast pouze pokud je zapnutÃ½
            if entry.options.get("enable_solar_forecast", False):
                key_entities.append(f"sensor.oig_{inverter_sn}_solar_forecast")

            # PÅ™idat battery_forecast pouze pokud je zapnutÃ½
            if entry.options.get("enable_battery_prediction", False):
                key_entities.append(f"sensor.oig_{inverter_sn}_battery_forecast")

            for entity_id in key_entities:
                entity_state = hass.states.get(entity_id)
                if entity_state:
                    _LOGGER.debug(
                        f"Dashboard entity check: {entity_id} = {entity_state.state}"
                    )
                else:
                    # DEBUG mÃ­sto WARNING - entity mÅ¯Å¾e chybÄ›t pÅ™i startu (timing issue)
                    _LOGGER.debug("Dashboard entity not yet available: %s", entity_id)
        else:
            _LOGGER.warning("Dashboard: No coordinator data for entity checking")

    except Exception as e:
        _LOGGER.error("Failed to setup frontend panel: %s", e)


async def _remove_frontend_panel(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """OdebrÃ¡nÃ­ frontend panelu."""
    try:
        panel_id = f"oig_cloud_dashboard_{entry.entry_id}"

        from homeassistant.components import frontend

        # OPRAVA: DÅ¯kladnÄ›jÅ¡Ã­ kontrola existence panelu
        panels_exist = False
        try:
            # ZkusÃ­me zÃ­skat pÅ™Ã­stup k registrovanÃ½m panelÅ¯m
            if hasattr(hass, "components") and "frontend" in hass.components:
                frontend_component = hass.components.frontend
                if hasattr(frontend_component, "async_register_built_in_panel"):
                    # Panel systÃ©m je dostupnÃ½
                    panels_exist = True
        except Exception:
            pass

        if not panels_exist:
            _LOGGER.debug(
                f"Frontend panel system not available, skipping removal of {panel_id}"
            )
            return

        # OPRAVA: Kontrola existence panelu pÅ™ed odstranÄ›nÃ­m
        try:
            # PokusÃ­me se zÃ­skat informace o panelu pÅ™ed jeho odstranÄ›nÃ­m
            # TÃ­m ovÄ›Å™Ã­me, Å¾e skuteÄnÄ› existuje
            panel_exists = False

            if hasattr(hass.data.get("frontend_panels", {}), panel_id):
                panel_exists = True
            elif hasattr(hass.data.get("frontend", {}), "panels"):
                existing_panels = hass.data["frontend"].panels
                panel_exists = panel_id in existing_panels

            if not panel_exists:
                _LOGGER.debug("Panel %s doesn't exist, nothing to remove", panel_id)
                return
        except Exception as check_error:
            _LOGGER.debug(
                f"Cannot check panel existence, proceeding with removal attempt: {check_error}"
            )

        # Pokus o odebrÃ¡nÃ­ panelu
        if hasattr(frontend, "async_remove_panel") and callable(
            getattr(frontend, "async_remove_panel")
        ):
            try:
                await frontend.async_remove_panel(hass, panel_id)
                _LOGGER.info("âœ… Panel removed: %s", panel_id)
            except ValueError as ve:
                if "unknown panel" in str(ve).lower():
                    _LOGGER.debug(
                        f"Panel {panel_id} was already removed or never existed"
                    )
                else:
                    _LOGGER.warning("Error removing panel %s: {ve}", panel_id)
            except Exception as re:
                _LOGGER.debug("Panel removal handled (panel may not exist): %s", re)
        else:
            _LOGGER.debug("async_remove_panel not available")

    except Exception as e:
        # OPRAVA: VÅ¡echny chyby logujeme jako debug, protoÅ¾e jsou oÄekÃ¡vanÃ©
        _LOGGER.debug("Panel removal handled gracefully: %s", e)


async def _migrate_entity_unique_ids(hass: HomeAssistant, entry: ConfigEntry) -> None:  # noqa: C901
    """Migrace unique_id a cleanup duplicitnÃ­ch entit s _2, _3, atd."""
    _LOGGER.info("ðŸ” Starting _migrate_entity_unique_ids function...")
    from homeassistant.helpers import entity_registry as er
    import re

    entity_registry = er.async_get(hass)

    # Najdeme vÅ¡echny OIG entity pro tento config entry
    entities = er.async_entries_for_config_entry(entity_registry, entry.entry_id)
    _LOGGER.info(f"ðŸ“Š Found {len(entities)} entities for config entry")

    migrated_count = 0
    skipped_count = 0
    removed_count = 0
    enabled_count = 0
    renamed_count = 0

    # Projdeme vÅ¡echny entity a upravÃ­me je
    for entity in entities:
        old_unique_id = entity.unique_id
        entity_id = entity.entity_id
        duplicate_pattern = re.compile(r"^(.+?)(_\d+)$")

        # OPRAVA: PÅ™eskoÄit bojler senzory - majÃ­ vlastnÃ­ formÃ¡t unique_id bez oig_cloud_ prefixu
        # FormÃ¡t: {entry_id}_boiler_{sensor_type}
        if "_boiler_" in old_unique_id:
            skipped_count += 1
            _LOGGER.debug("Skipping boiler sensor (correct format): %s", entity_id)
            continue

        # 1. Pokud mÃ¡ entita sprÃ¡vnÃ½ formÃ¡t unique_id (oig_cloud_*):
        if old_unique_id.startswith("oig_cloud_"):
            # Zkontrolujeme, jestli entity_id mÃ¡ pÅ™Ã­ponu, ale unique_id ne
            entity_id_match = duplicate_pattern.match(entity_id)
            if entity_id_match:
                suffix = entity_id_match.group(2)
                base_entity_id = entity_id_match.group(1)

                # Pokud unique_id nemÃ¡ pÅ™Ã­ponu, ale entity_id ano, pÅ™ejmenujeme
                if not old_unique_id.endswith(suffix):
                    try:
                        # ZkusÃ­me pÅ™ejmenovat entity_id (odstranÃ­me pÅ™Ã­ponu)
                        entity_registry.async_update_entity(
                            entity_id, new_entity_id=base_entity_id
                        )
                        renamed_count += 1
                        _LOGGER.info(
                            f"ðŸ”„ Renamed entity: {entity_id} -> {base_entity_id}"
                        )
                        entity_id = base_entity_id  # Aktualizujeme pro dalÅ¡Ã­ kontroly
                    except Exception:
                        _LOGGER.warning("âš ï¸ Failed to rename %s: {e}", entity_id)

            # Pokud je disabled, enable ji
            if entity.disabled_by == er.RegistryEntryDisabler.INTEGRATION:
                try:
                    entity_registry.async_update_entity(entity_id, disabled_by=None)
                    enabled_count += 1
                    _LOGGER.info("âœ… Re-enabled correct entity: %s", entity_id)
                except Exception:
                    _LOGGER.warning("âš ï¸ Failed to enable %s: {e}", entity_id)

            skipped_count += 1
            continue

        # 2. MÃ¡ starÃ½ formÃ¡t unique_id - potÅ™ebuje migraci
        # ZjistÃ­me, jestli entity_id mÃ¡ pÅ™Ã­ponu _X (znamenÃ¡ duplicitu)
        entity_id_match = duplicate_pattern.match(entity_id)

        if entity_id_match:
            suffix = entity_id_match.group(2)  # napÅ™. "_2", "_3"

            # Pokud unique_id nemÃ¡ pÅ™Ã­ponu, ale entity_id ano = duplicita
            # Tyto entity SMAÅ½EME (ne jen disable)
            if not old_unique_id.endswith(suffix):
                try:
                    entity_registry.async_remove(entity_id)
                    removed_count += 1
                    _LOGGER.info(
                        f"ðŸ—‘ï¸ Removed duplicate entity: {entity_id} "
                        f"(unique_id={old_unique_id} doesn't match entity_id suffix)"
                    )
                    continue
                except Exception:
                    _LOGGER.warning("âš ï¸ Failed to remove %s: {e}", entity_id)
                    continue

        # 3. Migrace unique_id na novÃ½ formÃ¡t
        if old_unique_id.startswith("oig_") and not old_unique_id.startswith(
            "oig_cloud_"
        ):
            # FormÃ¡t oig_{boxId}_{sensor} -> oig_cloud_{boxId}_{sensor}
            new_unique_id = f"oig_cloud_{old_unique_id[4:]}"
        else:
            # FormÃ¡t {boxId}_{sensor} -> oig_cloud_{boxId}_{sensor}
            new_unique_id = f"oig_cloud_{old_unique_id}"

        try:
            entity_registry.async_update_entity(entity_id, new_unique_id=new_unique_id)
            migrated_count += 1
            _LOGGER.info(
                f"âœ… Migrated entity {entity_id}: {old_unique_id} -> {new_unique_id}"
            )
        except Exception:
            _LOGGER.warning("âš ï¸ Failed to migrate %s: {e}", entity_id)

        # PÅ™eskoÄÃ­me entity, kterÃ© uÅ¾ majÃ­ sprÃ¡vnÃ½ formÃ¡t
        if old_unique_id.startswith("oig_cloud_"):
            skipped_count += 1
            continue

        # Migrace formÃ¡tÅ¯ unique_id
        if old_unique_id.startswith("oig_") and not old_unique_id.startswith(
            "oig_cloud_"
        ):
            # FormÃ¡t oig_{boxId}_{sensor} -> oig_cloud_{boxId}_{sensor}
            new_unique_id = f"oig_cloud_{old_unique_id[4:]}"
        else:
            # FormÃ¡t {boxId}_{sensor} -> oig_cloud_{boxId}_{sensor}
            new_unique_id = f"oig_cloud_{old_unique_id}"

        try:
            entity_registry.async_update_entity(
                entity.entity_id, new_unique_id=new_unique_id
            )
            migrated_count += 1
            _LOGGER.info(
                f"âœ… Migrated entity {entity.entity_id}: {old_unique_id} -> {new_unique_id}"
            )
        except Exception:
            _LOGGER.warning("âš ï¸ Failed to migrate %s: {e}", entity_id)

    # Summary
    _LOGGER.info(
        f"ðŸ“Š Migration summary: migrated={migrated_count}, removed={removed_count}, "
        f"renamed={renamed_count}, enabled={enabled_count}, skipped={skipped_count}"
    )

    if removed_count > 0 or migrated_count > 0 or renamed_count > 0:
        message_parts = []

        if renamed_count > 0:
            message_parts.append(
                f"**PÅ™ejmenovÃ¡no {renamed_count} entit**\n"
                f"Entity s pÅ™Ã­ponami (_2, _3) byly pÅ™ejmenovÃ¡ny na sprÃ¡vnÃ© nÃ¡zvy.\n\n"
            )

        if removed_count > 0:
            message_parts.append(
                f"**OdstranÄ›no {removed_count} duplicitnÃ­ch entit**\n"
                f"Byly to starÃ© kolize s nesprÃ¡vnÃ½m unique_id.\n\n"
            )

        if migrated_count > 0:
            message_parts.append(
                f"**MigrovÃ¡no {migrated_count} entit na novÃ½ formÃ¡t unique_id**\n"
                f"VÅ¡echny OIG entity nynÃ­ pouÅ¾Ã­vajÃ­ standardnÃ­ formÃ¡t `oig_cloud_*`.\n\n"
            )

        if enabled_count > 0:
            message_parts.append(
                f"**Povoleno {enabled_count} sprÃ¡vnÃ½ch entit**\n"
                f"Entity s novÃ½m formÃ¡tem byly znovu aktivovÃ¡ny.\n\n"
            )

        message_parts.append(
            "**Co se stalo:**\n"
            "- StarÃ© entity se pÅ™eregistrovaly s novÃ½m unique_id\n"
            "- Duplicity byly odstranÄ›ny\n"
            "- VÅ¡echny entity by mÄ›ly fungovat normÃ¡lnÄ›\n\n"
            "**Pokud nÄ›co nefunguje:**\n"
            "Reload integrace v NastavenÃ­ â†’ ZaÅ™Ã­zenÃ­ & SluÅ¾by â†’ OIG Cloud\n\n"
            "Toto je jednorÃ¡zovÃ¡ migrace po aktualizaci integrace."
        )

        await hass.services.async_call(
            "persistent_notification",
            "create",
            {
                "title": "OIG Cloud: Migrace entit dokonÄena",
                "message": "".join(message_parts),
                "notification_id": "oig_cloud_migration_complete",
            },
        )

    if renamed_count > 0:
        _LOGGER.info("ðŸ”„ Renamed %s entities to correct entity_id", renamed_count)
    if migrated_count > 0:
        _LOGGER.info("ðŸ”„ Migrated %s entities to new unique_id format", migrated_count)
    if removed_count > 0:
        _LOGGER.warning("ðŸ—‘ï¸ Removed %s duplicate entities", removed_count)
    if enabled_count > 0:
        _LOGGER.info("âœ… Re-enabled %s correct entities", enabled_count)
    if skipped_count > 0:
        _LOGGER.debug(
            "â­ï¸ Skipped %s entities (already in correct format)", skipped_count
        )


async def async_setup_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:  # noqa: C901
    """Set up OIG Cloud from a config entry."""
    _LOGGER.info("oig_cloud: async_setup_entry started for entry_id=%s", entry.entry_id)
    _LOGGER.info(f"Setting up OIG Cloud entry: {entry.title}")
    _LOGGER.debug(f"Config data keys: {list(entry.data.keys())}")
    _LOGGER.debug(f"Config options keys: {list(entry.options.keys())}")

    # Inject defaults for new planner/autonomy options so legacy setups keep working
    _ensure_planner_option_defaults(hass, entry)
    _ensure_data_source_option_defaults(hass, entry)

    # MIGRACE 1: enable_spot_prices -> enable_pricing
    if "enable_spot_prices" in entry.options:
        _LOGGER.info("ðŸ”„ Migrating enable_spot_prices to enable_pricing")
        new_options = dict(entry.options)

        # Pokud enable_spot_prices byl True, zapneme enable_pricing
        if new_options.get("enable_spot_prices", False):
            new_options["enable_pricing"] = True
            _LOGGER.info("âœ… Migrated: enable_spot_prices=True -> enable_pricing=True")

        # OdstranÃ­me starÃ½ flag
        new_options.pop("enable_spot_prices", None)

        # Aktualizujeme entry
        hass.config_entries.async_update_entry(entry, options=new_options)
        _LOGGER.info("âœ… Migration completed - enable_spot_prices removed from config")

    # MIGRACE 2: Unique ID formÃ¡t pro vÅ¡echny entity
    _LOGGER.info("ðŸ”„ Starting entity unique_id migration...")
    try:
        await _migrate_entity_unique_ids(hass, entry)
        _LOGGER.info("âœ… Entity unique_id migration completed")
    except Exception as e:
        _LOGGER.error(f"âŒ Entity unique_id migration failed: {e}", exc_info=True)

    # Inicializace hass.data struktury pro tento entry PÅ˜ED pouÅ¾itÃ­m
    hass.data.setdefault(DOMAIN, {})
    hass.data[DOMAIN].setdefault(entry.entry_id, {})

    # Initialize data source state early so coordinator setup can respect local/hybrid modes.
    # Also try to infer box_id from local entities so local mapping works without cloud.
    init_data_source_state(hass, entry)
    try:
        options = dict(entry.options)
        if not options.get("box_id"):
            inferred = _infer_box_id_from_local_entities(hass)
            if inferred:
                options["box_id"] = inferred
                hass.config_entries.async_update_entry(entry, options=options)
                _LOGGER.info("Inferred box_id=%s from local entities", inferred)
    except Exception as err:
        _LOGGER.debug("Inferring box_id from local entities failed (non-critical): %s", err)

    # OPRAVA: Inicializujeme service_shield jako None pÅ™ed try blokem
    service_shield = None

    try:
        # Inicializujeme ServiceShield s entry parametrem
        from .service_shield import ServiceShield

        service_shield = ServiceShield(hass, entry)
        await service_shield.start()

        hass.data[DOMAIN][entry.entry_id]["service_shield"] = service_shield

        _LOGGER.info("ServiceShield inicializovÃ¡n a spuÅ¡tÄ›n")
    except Exception as e:
        _LOGGER.error("ServiceShield nenÃ­ dostupnÃ½ - obecnÃ¡ chyba: %s", e)
        # PokraÄujeme bez ServiceShield
        hass.data[DOMAIN][entry.entry_id]["service_shield"] = None
        # OPRAVA: UjistÃ­me se, Å¾e service_shield je None
        service_shield = None

    try:
        # NaÄtenÃ­ konfigurace z entry.data nebo entry.options
        username = entry.data.get(CONF_USERNAME) or entry.options.get(CONF_USERNAME)
        password = entry.data.get(CONF_PASSWORD) or entry.options.get(CONF_PASSWORD)

        # Debug log pro diagnostiku
        _LOGGER.debug(f"Username: {'***' if username else 'MISSING'}")
        _LOGGER.debug(f"Password: {'***' if password else 'MISSING'}")

        if not username or not password:
            _LOGGER.error("Username or password is missing from configuration")
            return False

        no_telemetry = entry.data.get(CONF_NO_TELEMETRY, False) or entry.options.get(
            CONF_NO_TELEMETRY, False
        )

        # OPRAVA: Preferuj options pÅ™ed data, jen pokud options neexistujÃ­, pouÅ¾ij data nebo default
        standard_scan_interval = entry.options.get(
            "standard_scan_interval"
        ) or entry.data.get(CONF_STANDARD_SCAN_INTERVAL, 30)
        extended_scan_interval = entry.options.get(
            "extended_scan_interval"
        ) or entry.data.get(CONF_EXTENDED_SCAN_INTERVAL, 300)

        _LOGGER.debug(
            f"Using intervals: standard={standard_scan_interval}s, extended={extended_scan_interval}s"
        )

        # DEBUG: DOÄŒASNÄš ZAKÃZAT telemetrii kvÅ¯li problÃ©mÅ¯m s vÃ½konem
        # OPRAVA: Telemetrie zpÅ¯sobovala nekoneÄnou smyÄku
        # if not no_telemetry:
        #     _LOGGER.debug("Telemetry enabled, setting up...")
        #     await _setup_telemetry(hass, username)
        # else:
        #     _LOGGER.debug("Telemetry disabled by configuration")

        _LOGGER.debug("Telemetry handled only by ServiceShield, not main module")

        # NOVÃ‰: VytvoÅ™enÃ­ OigCloudApi a zabalenÃ­ do SessionManageru
        oig_api = OigCloudApi(username, password, no_telemetry)

        _LOGGER.debug("Creating session manager wrapper")
        from .api.oig_cloud_session_manager import OigCloudSessionManager

        session_manager = OigCloudSessionManager(oig_api)

        # Respect local/hybrid mode: if local proxy is healthy, do not hit cloud during setup.
        # Session manager will authenticate on-demand if we later fall back to cloud.
        state = get_data_source_state(hass, entry.entry_id)
        should_check_cloud_now = state.effective_mode == DATA_SOURCE_CLOUD_ONLY

        if should_check_cloud_now:
            _LOGGER.debug("Initial authentication via session manager")
            # Session manager zavolÃ¡ authenticate() pÅ™i prvnÃ­m poÅ¾adavku,
            # ale provedeme to i zde pro kontrolu pÅ™ihlaÅ¡ovacÃ­ch ÃºdajÅ¯
            await session_manager._ensure_auth()

            # CRITICAL: Check if live data is enabled (actual element present in API response)
            # Stats structure: { "box_id": { "actual": {...}, "settings": {...} } }
            _LOGGER.debug("Kontrola, zda jsou v aplikaci OIG Cloud zapnutÃ¡ 'Å½ivÃ¡ data'...")
            try:
                test_stats = await oig_api.get_stats()
                if test_stats:
                    # Get first device data
                    first_device = next(iter(test_stats.values())) if test_stats else None
                    if not first_device or "actual" not in first_device:
                        _LOGGER.error(
                            "âŒ KRITICKÃ CHYBA: V aplikaci OIG Cloud nejsou zapnutÃ¡ 'Å½ivÃ¡ data'! "
                            "API odpovÄ›Ä neobsahuje element 'actual'. "
                            "UÅ¾ivatel musÃ­ v mobilnÃ­ aplikaci zapnout: NastavenÃ­ â†’ PÅ™Ã­stup k datÅ¯m â†’ Å½ivÃ¡ data"
                        )
                        raise ConfigEntryNotReady(
                            "V aplikaci OIG Cloud nejsou zapnutÃ¡ 'Å½ivÃ¡ data'. "
                            "ZapnÄ›te je v mobilnÃ­ aplikaci OIG Cloud (NastavenÃ­ â†’ PÅ™Ã­stup k datÅ¯m â†’ Å½ivÃ¡ data) "
                            "a restartujte Home Assistant."
                        )
                    _LOGGER.info(
                        "âœ… Kontrola Å¾ivÃ½ch dat ÃºspÄ›Å¡nÃ¡ - element 'actual' nalezen v API odpovÄ›di"
                    )
                else:
                    _LOGGER.warning(
                        "API vrÃ¡tilo prÃ¡zdnou odpovÄ›Ä, pÅ™eskakuji kontrolu Å¾ivÃ½ch dat"
                    )
            except ConfigEntryNotReady:
                raise
            except Exception as e:
                _LOGGER.warning("Nelze ovÄ›Å™it stav Å¾ivÃ½ch dat: %s", e)
                # PokraÄujeme i tak - mÅ¯Å¾e jÃ­t o doÄasnÃ½ problÃ©m s API
        else:
            _LOGGER.info(
                "Local telemetry mode active (configured=%s, local_ok=%s) â€“ skipping initial cloud authentication and live-data check",
                state.configured_mode,
                state.local_available,
            )

        # Inicializace koordinÃ¡toru - pouÅ¾ijeme session_manager.api (wrapper)
        coordinator = OigCloudCoordinator(
            hass, session_manager, standard_scan_interval, extended_scan_interval, entry
        )

        # OPRAVA: PoÄkej na prvnÃ­ data pÅ™ed vytvoÅ™enÃ­m senzorÅ¯
        _LOGGER.debug("Waiting for initial coordinator data...")
        await coordinator.async_config_entry_first_refresh()

        if coordinator.data is None:
            _LOGGER.error("Failed to get initial data from coordinator")
            raise ConfigEntryNotReady("No data received from OIG Cloud API")

        _LOGGER.debug(f"Coordinator data received: {len(coordinator.data)} devices")

        # Persist box_id once so sensors can be initialized without relying on coordinator.data order
        try:
            options = dict(entry.options)
            if not options.get("box_id") and coordinator.data:
                box_id = next(
                    (str(k) for k in coordinator.data.keys() if str(k).isdigit()),
                    None,
                )
                if box_id:
                    options["box_id"] = box_id
                    hass.config_entries.async_update_entry(entry, options=options)
                    _LOGGER.info("Persisted box_id=%s into config entry options", box_id)
        except Exception as err:
            _LOGGER.debug("Persisting box_id failed (non-critical): %s", err)

        # OPRAVA: Inicializace notification manageru se sprÃ¡vnÃ½m error handling
        notification_manager = None
        enable_cloud_notifications = entry.options.get("enable_cloud_notifications", True)
        cloud_active_for_setup = (
            get_data_source_state(hass, entry.entry_id).effective_mode
            == DATA_SOURCE_CLOUD_ONLY
        )
        if enable_cloud_notifications and cloud_active_for_setup:
            try:
                _LOGGER.debug("Initializing notification manager...")
                from .oig_cloud_notification import OigNotificationManager

                # PROBLÃ‰M: OvÄ›Å™Ã­me, Å¾e pouÅ¾Ã­vÃ¡me sprÃ¡vnÃ½ objekt
                _LOGGER.debug(f"Using API object: {type(oig_api)}")
                _LOGGER.debug(
                    f"API has get_notifications: {hasattr(oig_api, 'get_notifications')}"
                )

                # OPRAVA: PouÅ¾Ã­t oig_api objekt (OigCloudApi) mÃ­sto jakÃ©hokoliv jinÃ©ho
                # NOVÃ‰: PouÅ¾Ã­t session_manager.api pro pÅ™Ã­stup k underlying API
                notification_manager = OigNotificationManager(
                    hass, session_manager.api, "https://www.oigpower.cz/cez/"
                )

                # NastavÃ­me device_id z prvnÃ­ho dostupnÃ©ho zaÅ™Ã­zenÃ­ v coordinator.data
                if coordinator.data:
                    device_id = next(iter(coordinator.data.keys()))
                    notification_manager.set_device_id(device_id)
                    _LOGGER.debug("Set notification manager device_id to: %s", device_id)

                    # Inicializace Mode Transition Tracker
                    if service_shield:
                        try:
                            from .service_shield import ModeTransitionTracker

                            service_shield.mode_tracker = ModeTransitionTracker(
                                hass, device_id
                            )
                            await service_shield.mode_tracker.async_setup()
                            _LOGGER.info(
                                f"Mode Transition Tracker inicializovÃ¡n pro box {device_id}"
                            )
                        except Exception as tracker_error:
                            _LOGGER.warning(
                                f"Failed to initialize Mode Transition Tracker: {tracker_error}"
                            )
                            # PokraÄujeme bez trackeru

                    # OPRAVA: PouÅ¾Ã­t novÃ½ API pÅ™Ã­stup mÃ­sto fetch_notifications_and_status
                    try:
                        await notification_manager.update_from_api()
                        _LOGGER.debug("Initial notification data loaded successfully")
                    except Exception as fetch_error:
                        _LOGGER.warning(
                            f"Failed to fetch initial notifications (API endpoint may not exist): {fetch_error}"
                        )
                        # PokraÄujeme bez poÄÃ¡teÄnÃ­ch notifikacÃ­ - API endpoint moÅ¾nÃ¡ neexistuje

                    # PÅ™ipoj notification manager ke koordinÃ¡toru i kdyÅ¾ fetch selhal
                    # Manager mÅ¯Å¾e fungovat pozdÄ›ji pokud se API opravÃ­
                    coordinator.notification_manager = notification_manager
                    _LOGGER.info(
                        "Notification manager created and attached to coordinator (may not have data yet)"
                    )
                else:
                    _LOGGER.warning(
                        "No device data available, notification manager not initialized"
                    )
                    notification_manager = None

            except Exception as e:
                _LOGGER.warning(
                    f"Failed to setup notification manager (API may not be available): {e}"
                )
                # PokraÄujeme bez notification manageru - API endpoint moÅ¾nÃ¡ neexistuje nebo je nedostupnÃ½
                notification_manager = None
        else:
            _LOGGER.debug(
                "Cloud notifications disabled or cloud not active - skipping notification manager"
            )

        # Inicializace solar forecast (pokud je povolenÃ¡)
        solar_forecast = None
        if entry.options.get("enable_solar_forecast", False):
            try:
                _LOGGER.debug("Initializing solar forecast functionality")
                # Solar forecast se inicializuje pÅ™Ã­mo v sensorech, ne zde
                solar_forecast = {"enabled": True, "config": entry.options}
            except Exception as e:
                _LOGGER.error("Chyba pÅ™i inicializaci solÃ¡rnÃ­ pÅ™edpovÄ›di: %s", e)
                solar_forecast = {"enabled": False, "error": str(e)}

        # **OPRAVA: SprÃ¡vnÃ© nastavenÃ­ statistics pro reload**
        statistics_enabled = entry.options.get("enable_statistics", True)
        _LOGGER.debug("Statistics enabled: %s", statistics_enabled)

        # **OPRAVA: PÅ™idÃ¡nÃ­ analytics_device_info pro statistickÃ© senzory**
        analytics_device_info = {
            "identifiers": {(DOMAIN, f"{entry.entry_id}_analytics")},
            "name": "Analytics & Predictions",
            "manufacturer": "OIG Cloud",
            "model": "Analytics Module",
            "sw_version": "1.0",
        }

        # NOVÃ‰: Podpora pro OTE API a spotovÃ© ceny
        ote_api = None
        if entry.options.get("enable_pricing", False):
            try:
                _LOGGER.debug("Initializing OTE API for spot prices")
                from .api.ote_api import OteApi

                ote_api = OteApi()
                # OPRAVA: OdstranÄ›no volÃ¡nÃ­ fetch_spot_prices - data uÅ¾ jsou v coordinatoru
                _LOGGER.info("OTE API successfully initialized")
            except Exception as e:
                _LOGGER.error("Failed to initialize OTE API: %s", e)
                if ote_api:
                    await ote_api.close()
                ote_api = None
        else:
            _LOGGER.debug("Pricing disabled - skipping OTE API initialization")

        # NOVÃ‰: BojlerovÃ½ modul (pokud je povolen)
        boiler_coordinator = None
        if entry.options.get("enable_boiler", False):
            try:
                _LOGGER.debug("Initializing Boiler module")
                from .boiler.coordinator import BoilerCoordinator

                # Kombinace entry.data a entry.options pro config
                boiler_config = {**entry.data, **entry.options}

                boiler_coordinator = BoilerCoordinator(hass, boiler_config)

                # PrvnÃ­ refresh
                await boiler_coordinator.async_config_entry_first_refresh()

                _LOGGER.info("Boiler coordinator successfully initialized")
            except Exception as e:
                _LOGGER.error("Failed to initialize Boiler coordinator: %s", e)
                boiler_coordinator = None
        else:
            _LOGGER.debug("Boiler module disabled")

        # NOVÃ‰: PodmÃ­nÄ›nÃ© nastavenÃ­ dashboard podle konfigurace
        dashboard_enabled = entry.options.get(
            "enable_dashboard", False
        )  # OPRAVA: default False mÃ­sto True
        # OPRAVA: Dashboard registrujeme AÅ½ PO vytvoÅ™enÃ­ senzorÅ¯

        # TODO 3: Inicializace Balancing Manager (refactored - no physics)
        balancing_enabled = entry.options.get("balancing_enabled", True)
        _LOGGER.info("oig_cloud: balancing_enabled=%s", balancing_enabled)

        balancing_manager = None
        if balancing_enabled and BalancingManager is not None:
            try:
                _LOGGER.info("oig_cloud: Initializing BalancingManager")
                # Get box_id from coordinator data (not from entry.data)
                if coordinator.data:
                    box_id = next(iter(coordinator.data.keys()))
                else:
                    box_id = None
                    _LOGGER.warning(
                        "oig_cloud: No coordinator data available for box_id"
                    )

                storage_path = hass.config.path(".storage")

                balancing_manager = BalancingManager(hass, box_id, storage_path, entry)
                await balancing_manager.async_setup()

                _LOGGER.info("oig_cloud: BalancingManager successfully initialized")

                # PeriodickÃ© volÃ¡nÃ­ balancingu (check every 30min)
                from homeassistant.helpers.event import (
                    async_track_time_interval,
                    async_call_later,
                )
                from datetime import timedelta

                async def update_balancing(_now: Any) -> None:
                    """PeriodickÃ¡ kontrola balancingu."""
                    _LOGGER.debug("BalancingManager: periodic check_balancing()")
                    try:
                        await balancing_manager.check_balancing()
                    except Exception as e:
                        _LOGGER.error(f"Error checking balancing: {e}", exc_info=True)

                # Aktualizace kaÅ¾dÃ½ch 30 minut
                entry.async_on_unload(
                    async_track_time_interval(
                        hass, update_balancing, timedelta(minutes=30)
                    )
                )

                # PrvnÃ­ volÃ¡nÃ­ hned pÅ™i startu (po delay aby forecast mÄ›l Äas se inicializovat)
                async def initial_balancing_check(_now: Any) -> None:
                    """PoÄÃ¡teÄnÃ­ kontrola balancingu po startu."""
                    _LOGGER.debug("BalancingManager: initial check_balancing()")
                    try:
                        result = await balancing_manager.check_balancing()
                        if result:
                            _LOGGER.info(
                                f"âœ… Initial check created plan: {result.mode.name}"
                            )
                        else:
                            _LOGGER.debug("Initial check: no plan needed yet")
                    except Exception as e:
                        _LOGGER.error(
                            f"Error in initial balancing check: {e}", exc_info=True
                        )

                # PrvnÃ­ kontrola za 2 minuty (aby forecast mÄ›l Äas se inicializovat)
                async_call_later(hass, 120, initial_balancing_check)

            except Exception as err:
                _LOGGER.error(
                    "oig_cloud: Failed to initialize BalancingManager: %s",
                    err,
                    exc_info=True,
                )
                balancing_manager = None
        else:
            if not balancing_enabled:
                _LOGGER.info("oig_cloud: BalancingManager disabled via config options")
            if BalancingManager is None:
                _LOGGER.warning(
                    "oig_cloud: BalancingManager not available (import failed)"
                )

        # UloÅ¾enÃ­ dat do hass.data
        hass.data[DOMAIN][entry.entry_id] = {
            "coordinator": coordinator,
            "session_manager": session_manager,  # NOVÃ‰: UloÅ¾it session manager
            "notification_manager": notification_manager,
            "data_source_controller": None,
            "data_source_state": get_data_source_state(hass, entry.entry_id),
            "solar_forecast": solar_forecast,
            "statistics_enabled": statistics_enabled,
            "analytics_device_info": analytics_device_info,
            "service_shield": service_shield,
            "ote_api": ote_api,
            "boiler_coordinator": boiler_coordinator,  # NOVÃ‰: Boiler coordinator
            "balancing_manager": balancing_manager,  # TODO 3: Refactored Balancing Manager
            "dashboard_enabled": dashboard_enabled,  # NOVÃ‰: stav dashboard
            "config": {
                "enable_statistics": statistics_enabled,
                "enable_pricing": entry.options.get("enable_pricing", False),
                "enable_boiler": entry.options.get("enable_boiler", False),  # NOVÃ‰
                "enable_dashboard": dashboard_enabled,  # NOVÃ‰
            },
        }

        # Data source controller (cloud/hybrid/local with proxy health fallback)
        try:
            data_source_controller = DataSourceController(hass, entry, coordinator)
            await data_source_controller.async_start()
            hass.data[DOMAIN][entry.entry_id]["data_source_controller"] = (
                data_source_controller
            )
        except Exception as err:
            _LOGGER.warning("DataSourceController start failed (non-critical): %s", err)

        # OPRAVA: PÅ™idÃ¡nÃ­ ServiceShield dat do globÃ¡lnÃ­ho ÃºloÅ¾iÅ¡tÄ› pro senzory
        if service_shield:
            # VytvoÅ™Ã­me globÃ¡lnÃ­ odkaz na ServiceShield pro senzory
            hass.data[DOMAIN]["shield"] = service_shield

            # VytvoÅ™Ã­me device info pro ServiceShield
            shield_device_info = {
                "identifiers": {(DOMAIN, f"{entry.entry_id}_shield")},
                "name": "ServiceShield",
                "manufacturer": "OIG Cloud",
                "model": "Service Protection",
                "sw_version": "2.0",
            }
            hass.data[DOMAIN][entry.entry_id]["shield_device_info"] = shield_device_info

            _LOGGER.debug("ServiceShield data prepared for sensors")

            # OPRAVA: PÅ™idÃ¡nÃ­ debug logovÃ¡nÃ­ pro ServiceShield stav
            _LOGGER.info(f"ServiceShield status: {service_shield.get_shield_status()}")
            _LOGGER.info(f"ServiceShield queue info: {service_shield.get_queue_info()}")

        # VyÄiÅ¡tÄ›nÃ­ starÃ½ch/nepouÅ¾Ã­vanÃ½ch zaÅ™Ã­zenÃ­ pÅ™ed registracÃ­ novÃ½ch
        await _cleanup_unused_devices(hass, entry)

        # VÅ¾dy registrovat sensor platform
        await hass.config_entries.async_forward_entry_setups(entry, ["sensor"])

        # OPRAVA: Dashboard registrujeme aÅ¾ TERAZ - po vytvoÅ™enÃ­ vÅ¡ech senzorÅ¯ A POUZE pokud je enabled
        if dashboard_enabled:
            await _setup_frontend_panel(hass, entry)
            _LOGGER.info("OIG Cloud Dashboard panel enabled and registered")
        else:
            await _remove_frontend_panel(hass, entry)
            _LOGGER.info(
                "OIG Cloud Dashboard panel disabled - panel not registered"
            )  # OPRAVA: lepÅ¡Ã­ log message

        # PÅ™idÃ¡me listener pro zmÄ›ny konfigurace - OPRAVEN callback na async funkci
        entry.async_on_unload(entry.add_update_listener(async_update_options))

        # Async importy pro vyhnÃ¡nÃ­ se blokovÃ¡nÃ­ event loopu
        from .services import (
            async_setup_services,
            async_setup_entry_services_with_shield,
        )

        # Setup zÃ¡kladnÃ­ch sluÅ¾eb (pouze jednou pro celou integraci)
        if len([k for k in hass.data[DOMAIN].keys() if k != "shield"]) == 1:
            await async_setup_services(hass)

        # Setup entry-specific sluÅ¾eb s shield ochranou
        # OPRAVA: PÅ™edÃ¡nÃ­ service_shield pÅ™Ã­mo, ne z hass.data
        await async_setup_entry_services_with_shield(hass, entry, service_shield)

        # NOVÃ‰: Registrace HTTP API endpointÅ¯ pro boiler
        if boiler_coordinator:
            from .boiler.api_views import register_boiler_api_views

            register_boiler_api_views(hass)
            _LOGGER.info("Boiler API endpoints registered")

        # NOVÃ‰: Registrace Planning API endpointÅ¯
        from .api.planning_api import setup_planning_api_views

        setup_planning_api_views(hass)
        _LOGGER.info("Planning API endpoints registered")

        # NOVÃ‰: Registrace OIG Cloud REST API endpointÅ¯ pro heavy data
        # (timeline, spot prices, analytics)
        try:
            from .api.ha_rest_api import setup_api_endpoints

            setup_api_endpoints(hass)
            _LOGGER.info("âœ… OIG Cloud REST API endpoints registered successfully")
        except Exception as e:
            _LOGGER.error(
                f"Failed to register OIG Cloud REST API endpoints: {e}", exc_info=True
            )
            # PokraÄujeme i bez API - senzory budou fungovat s attributes

        # OPRAVA: Zajistit, Å¾e ServiceShield je pÅ™ipojenÃ½ k volÃ¡nÃ­ sluÅ¾eb
        if service_shield:
            _LOGGER.info(
                "ServiceShield je aktivnÃ­ a pÅ™ipravenÃ½ na interceptovÃ¡nÃ­ sluÅ¾eb"
            )
            # Test interceptu - simulace volÃ¡nÃ­ pro debug
            _LOGGER.debug(f"ServiceShield pending: {len(service_shield.pending)}")
            _LOGGER.debug(f"ServiceShield queue: {len(service_shield.queue)}")
            _LOGGER.debug(f"ServiceShield running: {service_shield.running}")

            # OPRAVA: ExplicitnÃ­ spuÅ¡tÄ›nÃ­ monitorovÃ¡nÃ­
            _LOGGER.debug("OvÄ›Å™uji, Å¾e ServiceShield monitoring bÄ›Å¾Ã­...")

            # PÅ™idÃ¡me test callback pro ovÄ›Å™enÃ­ funkÄnosti
            async def test_shield_monitoring(_now: Any) -> None:
                status = service_shield.get_shield_status()
                queue_info = service_shield.get_queue_info()
                _LOGGER.debug(
                    f"[OIG Shield] Test monitoring tick - pending: {len(service_shield.pending)}, queue: {len(service_shield.queue)}, running: {service_shield.running}"
                )
                _LOGGER.debug("[OIG Shield] Status: %s", status)
                _LOGGER.debug("[OIG Shield] Queue info: %s", queue_info)

                # OPRAVA: Debug telemetrie - ukÃ¡Å¾eme co by se odesÃ­lalo
                if service_shield.telemetry_handler:
                    _LOGGER.debug("[OIG Shield] Telemetry handler je aktivnÃ­")
                    if hasattr(service_shield, "_log_telemetry"):
                        _LOGGER.debug(
                            "[OIG Shield] Telemetry logging metoda je dostupnÃ¡"
                        )
                else:
                    _LOGGER.debug("[OIG Shield] Telemetry handler nenÃ­ aktivnÃ­")

            # Registrujeme test callback na kratÅ¡Ã­ interval pro debug
            from homeassistant.helpers.event import async_track_time_interval
            from datetime import timedelta

            entry.async_on_unload(
                async_track_time_interval(
                    hass, test_shield_monitoring, timedelta(seconds=30)
                )
            )

        else:
            _LOGGER.warning("ServiceShield nenÃ­ dostupnÃ½ - sluÅ¾by nebudou chrÃ¡nÄ›ny")

        # OPRAVA: ODSTRANÄšNÃ duplicitnÃ­ registrace sluÅ¾eb - zpÅ¯sobovala pÅ™epsÃ¡nÃ­ sprÃ¡vnÃ©ho schÃ©matu
        # SluÅ¾by se uÅ¾ registrovaly vÃ½Å¡e v async_setup_entry_services_with_shield
        # await services.async_setup_services(hass)  # ODSTRANÄšNO
        # await services.async_setup_entry_services(hass, entry)  # ODSTRANÄšNO

        _LOGGER.debug("OIG Cloud integration setup complete")
        return True

    except Exception as e:
        _LOGGER.error(f"Error initializing OIG Cloud: {e}", exc_info=True)
        raise ConfigEntryNotReady(f"Error initializing OIG Cloud: {e}") from e


async def _setup_telemetry(hass: core.HomeAssistant, username: str) -> None:
    """Setup telemetry if enabled."""
    try:
        _LOGGER.debug("Starting telemetry setup...")

        email_hash = hashlib.sha256(username.encode("utf-8")).hexdigest()
        hass_id = hashlib.sha256(hass.data["core.uuid"].encode("utf-8")).hexdigest()

        _LOGGER.debug(
            f"Telemetry identifiers - Email hash: {email_hash[:16]}..., HASS ID: {hass_id[:16]}..."
        )

        # PÅ™esuneme import do async executor aby neblokoval event loop
        def _import_and_setup_telemetry() -> Any:
            try:
                _LOGGER.debug("Importing REST telemetry modules...")
                from .shared.logging import setup_otel_logging

                _LOGGER.debug("Setting up REST telemetry logging...")
                handler = setup_otel_logging(email_hash, hass_id)

                # PÅ™idÃ¡me handler do root loggeru pro OIG Cloud
                oig_logger = logging.getLogger("custom_components.oig_cloud")
                oig_logger.addHandler(handler)
                oig_logger.setLevel(logging.DEBUG)

                _LOGGER.debug(
                    f"Telemetry handler attached to logger: {oig_logger.name}"
                )
                _LOGGER.info("REST telemetry successfully initialized")

                return handler
            except Exception as e:
                _LOGGER.error(f"Error in telemetry setup executor: {e}", exc_info=True)
                raise

        await hass.async_add_executor_job(_import_and_setup_telemetry)
        _LOGGER.debug("REST telemetry setup completed via executor")

        # Test log pro ovÄ›Å™enÃ­ funkÄnosti
        _LOGGER.info("TEST: Telemetry test message - this should appear in New Relic")

    except Exception as e:
        _LOGGER.warning(f"Failed to setup telemetry: {e}", exc_info=True)
        # PokraÄujeme bez telemetrie


async def async_unload_entry(
    hass: HomeAssistant, entry: config_entries.ConfigEntry
) -> bool:
    """Unload a config entry."""
    # OdebrÃ¡nÃ­ dashboard panelu pÅ™i unload
    await _remove_frontend_panel(hass, entry)

    # TODO 3: Cleanup Balancing Manager (no async_shutdown needed - just storage)

    # NOVÃ‰: Cleanup session manageru
    if DOMAIN in hass.data and entry.entry_id in hass.data[DOMAIN]:
        data_source_controller = hass.data[DOMAIN][entry.entry_id].get(
            "data_source_controller"
        )
        if data_source_controller:
            try:
                await data_source_controller.async_stop()
            except Exception as err:
                _LOGGER.debug("DataSourceController stop failed: %s", err)

        session_manager = hass.data[DOMAIN][entry.entry_id].get("session_manager")
        if session_manager:
            _LOGGER.debug("Closing session manager")
            await session_manager.close()

    unload_ok = await hass.config_entries.async_unload_platforms(entry, ["sensor"])
    if unload_ok:
        hass.data[DOMAIN].pop(entry.entry_id, None)
    return unload_ok


async def async_reload_entry(config_entry: config_entries.ConfigEntry) -> None:
    """Reload config entry."""
    hass = config_entry.hass
    await async_unload_entry(hass, config_entry)
    await async_setup_entry(hass, config_entry)


async def async_update_options(
    hass: HomeAssistant, config_entry: config_entries.ConfigEntry
) -> None:
    """Update options with dashboard management."""
    old_options = config_entry.options
    new_options = dict(config_entry.options)

    # Kontrola zmÄ›ny dashboard nastavenÃ­
    old_dashboard_enabled = old_options.get("enable_dashboard", False)
    new_dashboard_enabled = new_options.get("enable_dashboard", False)

    _LOGGER.debug(
        f"Dashboard options update: old={old_dashboard_enabled}, new={new_dashboard_enabled}"
    )

    if old_dashboard_enabled != new_dashboard_enabled:
        _LOGGER.info(
            f"Dashboard setting changed: {old_dashboard_enabled} -> {new_dashboard_enabled}"
        )

        if new_dashboard_enabled:
            # ZapnutÃ­ dashboard
            await _setup_frontend_panel(hass, config_entry)
            _LOGGER.info("Dashboard panel enabled")
        else:
            # VypnutÃ­ dashboard
            await _remove_frontend_panel(hass, config_entry)
            _LOGGER.info("Dashboard panel disabled")

        # Aktualizace dat v hass.data
        if DOMAIN in hass.data and config_entry.entry_id in hass.data[DOMAIN]:
            hass.data[DOMAIN][config_entry.entry_id][
                "dashboard_enabled"
            ] = new_dashboard_enabled
            hass.data[DOMAIN][config_entry.entry_id]["config"][
                "enable_dashboard"
            ] = new_dashboard_enabled
    else:
        # PÅ˜IDÃNO: I kdyÅ¾ se hodnota nezmÄ›nila, ujistÃ­me se Å¾e panel nenÃ­ registrovÃ¡n pokud je disabled
        if not new_dashboard_enabled:
            await _remove_frontend_panel(hass, config_entry)
            _LOGGER.debug("Ensuring dashboard panel is not registered (disabled)")

    # Pokud byla oznaÄena potÅ™eba reload, proveÄ ho
    if new_options.get("_needs_reload"):
        new_options.pop("_needs_reload", None)
        hass.config_entries.async_update_entry(config_entry, options=new_options)
        hass.async_create_task(hass.config_entries.async_reload(config_entry.entry_id))
    else:
        hass.config_entries.async_update_entry(config_entry, options=new_options)


async def _cleanup_unused_devices(hass: HomeAssistant, entry: ConfigEntry) -> None:
    """VyÄiÅ¡tÄ›nÃ­ nepouÅ¾Ã­vanÃ½ch zaÅ™Ã­zenÃ­."""
    try:
        from homeassistant.helpers import device_registry as dr
        from homeassistant.helpers import entity_registry as er

        device_registry = dr.async_get(hass)
        entity_registry = er.async_get(hass)

        # Najdeme vÅ¡echna zaÅ™Ã­zenÃ­ pro tuto integraci
        devices = dr.async_entries_for_config_entry(device_registry, entry.entry_id)

        devices_to_remove = []
        for device in devices:
            device_name = device.name or ""
            should_keep = True

            # Definujeme pravidla pro zachovÃ¡nÃ­ zaÅ™Ã­zenÃ­
            keep_patterns = [
                "OIG.*Statistics",  # StarÃ© statistiky (regex pattern)
                "ÄŒEZ Battery Box",
                "OIG Cloud Home",
                "Analytics & Predictions",
                "ServiceShield",
            ]
            for pattern in keep_patterns:
                if pattern in device_name:
                    should_keep = True
                    break
            else:
                # Pokud neodpovÃ­dÃ¡ keep patterns, zkontrolujeme remove patterns

                # Zkontrolujeme, jestli zaÅ™Ã­zenÃ­ odpovÃ­dÃ¡ keep patterns
                for pattern in keep_patterns:
                    if re.search(pattern, device_name):
                        should_keep = False
                        break
                else:
                    # Pokud nemÃ¡ Å¾Ã¡dnÃ© entity, mÅ¯Å¾eme smazat
                    device_entities = er.async_entries_for_device(
                        entity_registry, device.id
                    )
                    if not device_entities:
                        should_keep = False

            if not should_keep:
                devices_to_remove.append(device)
                _LOGGER.info(
                    f"Marking device for removal: {device.name} (ID: {device.id})"
                )
            else:
                _LOGGER.debug(f"Keeping device: {device.name} (ID: {device.id})")

        # SmaÅ¾eme nepouÅ¾Ã­vanÃ¡ zaÅ™Ã­zenÃ­
        for device in devices_to_remove:
            try:
                _LOGGER.info(f"Removing unused device: {device.name} (ID: {device.id})")
                device_registry.async_remove_device(device.id)
            except Exception as e:
                _LOGGER.warning("Error removing device {device.id}: %s", e)

        if devices_to_remove:
            _LOGGER.info(f"Removed {len(devices_to_remove)} unused devices")
        else:
            _LOGGER.debug("No unused devices found to remove")
    except Exception as e:
        _LOGGER.warning("Error cleaning up devices: %s", e)
